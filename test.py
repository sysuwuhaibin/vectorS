import streamlit as st
from milvus_utility import MilvusUtility
from openai_utility import ChatBot
import openai
import numpy as np
import os
import configparser
import logging

st.set_page_config(page_title="ğŸ’¬ Tiancom VectorS",layout="wide")

with st.sidebar:
    st.title('ğŸ’¬ Tiancom VectorS')
    if 'OPENAI_BASE_URL' in st.secrets:
        text = 'OPENAI_BASE_URL\n' + st.secrets['OPENAI_BASE_URL']
        st.success(text, icon='âœ…')
        openai_base_url = st.secrets['OPENAI_BASE_URL']
    else:
        openai_base_url = st.text_input('è¯·è¾“å…¥OPENAI BASE URL:', type='default')
        if not (openai_base_url.startswith('http') and len(openai_base_url) > 10):
            st.warning('è¯·è¾“å…¥æ­£ç¡®çš„OPENAI BASE URLï¼', icon='âš ï¸')
        else:
            st.success('OPENAI BASE URLå·²è¾“å…¥ï¼', icon='âœ…')
    os.environ['OPENAI_BASE_URL'] = openai_base_url
    if 'OPENAI_API_TOKEN' in st.secrets:
        st.success('API keyå·²ç»æä¾›!', icon='âœ…')
        openai_api = st.secrets['OPENAI_API_TOKEN']
    else:
        openai_api = st.text_input('è¯·è¾“å…¥OPENAI API KEY:', type='password')
        if not (openai_api.startswith('sk-') and len(openai_api)==51):
            st.warning('è¯·è¾“å…¥æ­£ç¡®çš„OPENAI API KEYï¼', icon='âš ï¸')
        else:
            st.success('OPENAI API keyå·²è¾“å…¥', icon='âœ…')
    os.environ['OPENAI_API_TOKEN'] = openai_api

    #st.markdown('ğŸ“– Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')
    recommend_degree = st.slider('æ¨èç¨‹åº¦è®¾ç½®ï¼š', 0.0, 1.0, 0.75)

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä½ çš„è§„åˆ’å’¨è¯¢å°åŠ©æ‰‹VectorSï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œä½ ä»¬å­˜åœ¨çš„é—®é¢˜ã€‚æˆ‘å°†ä¼šä¸ºä½ æŒ‡å‡ºé—®é¢˜çš„ç±»å‹åŠå…¶å»ºè®®å“¦ï¼"}]
st.sidebar.button('æ¸…é™¤èŠå¤©å†å²', on_click=clear_chat_history)

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler('app.log', encoding='utf-8')
file_handler.setLevel(logging.INFO)

# åˆ›å»ºæ—¥å¿—æ ¼å¼å™¨
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

# å°†æ–‡ä»¶å¤„ç†å™¨æ·»åŠ åˆ°æ—¥å¿—è®°å½•å™¨
logger.addHandler(file_handler)

# åˆ›å»ºä¸€ä¸ªConfigParserå¯¹è±¡
config = configparser.ConfigParser()

# åˆ›å»ºä¸€ä¸ªConfigParserå¯¹è±¡
config = configparser.ConfigParser()

# è¯»å–INIæ–‡ä»¶
config.read('setting.ini', encoding='utf-8')

# é¢„å¤„ç†æç¤ºçš„å‡½æ•°
def preprocess_prompt(promt_embedding_res, text, namespace):
    try:
        prompt_final = {
            "system": '',
            "user": ''
        }
        if config.get('Model', 'selected_model') == 'Pinecone':
            prompt_res = PineconeUtility().query_pinecone(promt_embedding_res, namespace)
            logger.info("ä»PineconeçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ï¼šå®Œæˆ")
            # é‡æ–°æ„é€ æç¤º
            contexts1 = ["\nã€ç›¸å…³åº¦ã€‘" + str(item['score']) + "\nã€å€™é€‰ç»“æœ" + str(i + 1) + "ã€‘\n" + item['metadata'][
                'data'].lstrip('') for
                         i, item in enumerate(prompt_res['matches'])]
            contexts = [item['metadata']['data'] for item in prompt_res['matches']]
            result = "\nã€æŸ¥è¯¢é—®é¢˜ã€‘ " + text + "\n===================================================" + \
                     "\n===================================================".join(contexts1) + "\n"
            prompt_final['system'] = ''.join(contexts)
            prompt_final['user'] = text
            logger.info("é‡æ–°æ„é€ æç¤ºï¼šå®Œæˆ")
        elif config.get('Model', 'selected_model') == 'Milvus':
            Milvus_TOP_K = config.getint('Milvus', 'top_k')
            prompt_res = MilvusUtility().search_entity("GUIHUAZHIXUN", promt_embedding_res, ["classification", "description", "content"])
            print(prompt_res)
            logger.info("ä»MilvusçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³å†…å®¹ï¼šå®Œæˆ")
            # é‡æ–°æ„é€ æç¤º
            contexts1 = [" å€™é€‰ç»“æœ" + str(i + 1) + "   ç›¸å…³åº¦:" + str(1 - item.distance) + " =======================\n" + \
                         'ã€é—®é¢˜åˆ†ç±»ã€‘' + item.entity.get('classification') + '\nã€é—®é¢˜æ ‡é¢˜ã€‘' + item.entity.get(
                'description') + '\nã€é—®é¢˜æè¿°ã€‘' + item.entity.get('content') for i, item in enumerate(prompt_res[0])]
            contexts = ["å¯¹ä¸èµ·ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç¬¦åˆæ‚¨çš„é—®é¢˜çš„å»ºè®®ï¼"]
            if float(1 - prompt_res[0][0].distance) > recommend_degree:
                contexts = ['\nã€æ¨èç¨‹åº¦ã€‘' + str(1 - item.distance) + '\n\nã€é—®é¢˜åˆ†ç±»ã€‘' + item.entity.get('classification') + '\n\nã€é—®é¢˜æ ‡é¢˜ã€‘' + item.entity.get(
                    'description') + '\n\nã€é—®é¢˜æè¿°ã€‘' + item.entity.get('content') for item in prompt_res[0]]
            result = "\nã€æŸ¥è¯¢é—®é¢˜ã€‘ " + text + "\n=======================" + \
                     "\n=======================".join(contexts1) + "\n\n"
            prompt_final['system'] = ' '.join(contexts)
            prompt_final['user'] = text
            logger.info("é‡æ–°æ„é€ æç¤ºï¼šå®Œæˆ")
        return prompt_final, result

    except Exception as e:
        logger.error("é¢„å¤„ç†æç¤ºæ—¶å‡ºç°å¼‚å¸¸:", str(e))
        return None, None

openai.api_base = "http://ai.hellopas.com:3000/v1"
openai.api_key = "sk-2bH7CNR4jC3ZL00MF6BfFf5848A74c64A09c4d4eFeAf2d65"

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯ä½ çš„è§„åˆ’å’¨è¯¢å°åŠ©æ‰‹VectorSï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œä½ ä»¬å­˜åœ¨çš„é—®é¢˜ã€‚æˆ‘å°†ä¼šä¸ºä½ æŒ‡å‡ºé—®é¢˜çš„ç±»å‹åŠå…¶å»ºè®®å“¦ï¼"}]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


prompt = st.chat_input("è¯·è¾“å…¥")

if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

def chat(text):
    response = openai.ChatCompletion.create(
        model='gpt-3.5-turbo',
        messages=[
            {'role': 'user', 'content': text}
        ],
        temperature=0
    )

    return response.choices[0].message.content

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("æ­£åœ¨æ€è€ƒä¸­..."):
            embedding_res = ChatBot().generate_embedding(prompt)
            prompt_final, result = preprocess_prompt(embedding_res, prompt, namespace="dddd")
            if prompt_final is not None and prompt_final['system'] != 'å¯¹ä¸èµ·ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç¬¦åˆæ‚¨çš„é—®é¢˜çš„å»ºè®®ï¼':
                completion = ChatBot().interact_with_llm(prompt_final)
                response = prompt_final['system'] + '\n\n' + completion
            elif prompt_final is None:
                response = 'å½“å‰æœåŠ¡ä¸å¯ç”¨ï¼Œå¾ˆæŠ±æ­‰ï¼'
            else:
                response = 'å¯¹ä¸èµ·ï¼ŒçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç¬¦åˆæ‚¨çš„é—®é¢˜çš„å»ºè®®ï¼'
            placeholder = st.empty()
            full_response = response
            placeholder.markdown(full_response, unsafe_allow_html=True)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)
    st.balloons()
